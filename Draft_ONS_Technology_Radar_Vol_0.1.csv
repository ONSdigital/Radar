name,ring,quadrant,isNew,description
Four key metrics,Adopt,Techniques,FALSE,"<p>To measure software delivery performance, more and more organizations are defaulting to the <strong>four key metrics</strong> as defined by the <a href=""https://www.devops-research.com/"">DORA research</a> program: change lead time, deployment frequency, mean time to restore (MTTR) and change fail percentage. This research and its statistical analysis have shown a clear link between high-delivery performance and these metrics; they provide a great leading indicator for how a delivery organization as a whole is doing.</p>

<p>We're still big proponents of these metrics, but we've also learned some lessons. We're still observing misguided approaches with tools that help teams measure these metrics based purely on their continuous delivery (CD) pipelines. In particular when it comes to the stability metrics (MTTR and change fail percentage), CD pipeline data alone doesn't provide enough information to determine what a deployment failure with real user impact is. Stability metrics only make sense if they include data about real incidents that degrade service for the users.</p>

<p>We recommend always to keep in mind the ultimate intention behind a measurement and use it to reflect and learn. For example, before spending weeks building up sophisticated dashboard tooling, consider just regularly taking the <a href=""https://www.devops-research.com/quickcheck.html"">DORA quick check</a> in team retrospectives. This gives the team the opportunity to reflect on which <a href=""https://www.devops-research.com/research.html#capabilities"">capabilities</a> they could work on to improve their metrics, which can be much more effective than overdetailed out-of-the-box tooling. Keep in mind that these four key metrics originated out of the organization-level research of high-performing teams, and the use of these metrics at a team level should be a way to reflect on their own behaviors, not just another set of metrics to add to the dashboard.</p>"
GitHub Actions,Trial,ONS Platforms,FALSE,"<p><strong><a href=""https://docs.github.com/en/actions"">GitHub Actions</a></strong> has grown considerably last year. It has proven that it can take on more complex workflows and call other actions in composite actions among other things. It still has some shortcomings, though, such as its inability to re-trigger a single job of a workflow. Although the ecosystem in the <a href=""https://github.com/marketplace?type=actions"">GitHub Marketplace</a> has its obvious advantages, giving third-party GitHub Actions access to your build pipeline risks sharing secrets in insecure ways (we recommend following GitHub's advice on <a href=""https://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions"">security hardening</a>). However, the convenience of creating your build workflow directly in GitHub next to your source code combined with the ability to run GitHub Actions locally using open-source tools such as <a href=""https://github.com/nektos/act"">act</a> is a compelling option that has facilitated setup and onboarding of our teams.</p>"
GitLab CI/CD,Trial,ONS Platforms,TRUE,"<p>If you're using <a href=""https://gitlab.com/"">GitLab</a> to manage your software delivery, you should also look at <strong><a href=""https://docs.gitlab.com/ee/ci/"">GitLab CI/CD</a></strong> for your continuous integration and continuous delivery needs. We've found it especially useful when used with on-premise GitLab and self-hosted runners, as this combination gets around authorization headaches often caused by using a cloud-based solution. Self-hosted runners can be fully configured for your purposes with the right OS and dependencies installed, and as a result pipelines can run much faster than using a cloud-provisioned runner that needs to be configured each time.</p>

<p>Apart from the basic build, test and deploy pipeline, GitLab's product supports Services, Auto Devops and ChatOps among other advanced features. Services are useful in running Docker services such as Postgres or <a href=""/radar/languages-and-frameworks/testcontainers"">Testcontainer</a> linked to a job for integration and end-to-end testing. Auto Devops creates pipelines with zero configuration which is very useful for teams that are new to continuous delivery or for organizations with many repositories that would otherwise need to create many pipelines manually.</p>"
SPP,Adopt,ONS Platforms,TRUE,"<P> The data cleaning services is part of SPP, focus on necessary validation checks on the survey data alllowing changes to the survey response </p> <p> <b>Technology Stack</b><br>Python,Faregate, GraphQL, PostGres, Terraform, GitHub, AWS, KMS Security Manager</p>"
tfsec,Adopt,Tools,FALSE,"<p>For our projects using <a href=""/radar/tools/terraform"">Terraform</a>, <strong><a href=""https://github.com/liamg/tfsec"">tfsec</a></strong> has quickly become a default static analysis tool to detect potential security risks. It's easy to integrate into a CI pipeline and has a growing library of checks against all of the major cloud providers and platforms like Kubernetes. Given its ease of use, we believe tfsec could be a good addition to any Terraform project.</p>"
Terraform Validator,Assess,Tools,TRUE,"<p>Organizations that have adopted <a href=""/radar/techniques/infrastructure-as-code"">infrastructure as code</a> and self-service infrastructure platforms are looking for ways to give teams a maximum of autonomy while still enforcing good security practices and organizational policies. We've highlighted <a href=""/radar/tools/tfsec"">tfsec</a> before and are moving it into the Adopt category in this Radar. For teams working on GCP, <a href=""https://github.com/GoogleCloudPlatform/terraform-validator""><strong>Terraform Validator</strong></a> could be an option when creating a policy library, a set of constraints that are checked against Terraform configurations.</p>"
Testcontainers,Adopt,languages-and-frameworks,FALSE,"<p>We've had enough experience with <strong><a href=""https://www.testcontainers.org/"">Testcontainers</a></strong> that we think it's a useful default option for creating a reliable environment for running tests. It's a library, ported to <a href=""https://github.com/testcontainers"">multiple languages</a>, that Dockerizes common test dependencies — including various types of databases, queuing technologies, cloud services and UI testing dependencies like web browsers — with the ability to run custom Dockerfiles when needed. It works well with test frameworks like JUnit, is flexible enough to let users manage the container lifecycle and advanced networking and quickly sets up an integrated test environment. Our teams have consistently found this library of programmable, lightweight and disposable containers to make functional tests more reliable.</p>"
